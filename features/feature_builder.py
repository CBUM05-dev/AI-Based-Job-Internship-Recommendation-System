# features/feature_builder.py
# New FeatureBuilder (Embedding-Based)
# ğŸ‘‰ Sentence Transformers = des modÃ¨les dâ€™EMBEDDINGS


import pandas as pd
from sentence_transformers import SentenceTransformer
from utils.logger import get_logger

logger = get_logger("FeatureBuilder")

embedding_model = "all-MiniLM-L6-v2"

class FeatureBuilder :
    def __init__(self , model_name = embedding_model):
        """
        Embedding-based Feature Builder (open-world)
        """
        logger.info(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("Model loaded succesfully")
        
    # ------------------------
    # Text builders
    # ------------------------
    def user_to_text(self , user: dict) -> str:
        return f"""
        Skills: {", ".join(user.get('skills' , []))}
        Level: {user.get('level' , '')}
        Mode: {user.get('mode' , '')} 
        Domain: {user.get('domain','')}
        """.strip()
        
    def job_to_text(self, job: dict) -> str:
        return f"""
        Title: {job.get('title' , '')}
        Skills: {job.get('skills' , '')}
        Level: {job.get('level' , '')}
        Mode: {job.get('mode', '')}
        Domain: {job.get('domain' , '')}
        """.strip()    
        
    # ------------------------
    # Embedding methods
    # ------------------------
    def transform_users(self , users_df: pd.DataFrame):
        logger.info("Embedding users")
        texts = users_df.apply(
            lambda row : self.user_to_text(row.to_dict()) , 1
        ).tolist()
        
        embeddings = self.model.encode(texts , show_progress_bar=True)
        return embeddings
    
    def transform_jobs(self , jobs_df : pd.DataFrame):
        logger.info("Embedding jobs")
        texts = jobs_df.apply(
            lambda row: self.job_to_text(row.to_dict()) , axis=1
        ).tolist()
        
        embeddings = self.model.encode(texts , show_progress_bar = True , normalize_embeddings=True)
        return embeddings
        


        """
Hybrid Recommender :
So you KEEP BOTH:

ğŸ”¹ Embeddings â†’ semantic match

ğŸ”¹ Level / mode / domain â†’ control & explainability
        
ğŸ”‘ Point CRUCIAL :
Le modÃ¨le impose la taille du vecteur
384 = dÃ©pend du modÃ¨le (all-MiniLM-L6-v2)
User embedding â†’ 384 dims

Job embedding â†’ 384 dims
âœ”ï¸ toujours comparables

5ï¸âƒ£ model.encode(texts)

Input : List[str] ou str

Output : np.ndarray shape (N, 384)

ğŸ’¥ Parfait pour cosine similarity


âš ï¸ Petit dÃ©tail Ã  amÃ©liorer (niveau ingÃ©nieur)

Ajoute Ã§a pour Ãªtre clean :

embeddings = self.model.encode(
    texts,
    show_progress_bar=True,
    normalize_embeddings=True
)


ğŸ‘‰ Ã‡a te permet :

cosine similarity = simple dot product

plus stable numÃ©riquement

ğŸ”‘ Point clÃ© (Ã  graver)

normalize_embeddings=True
âŒ ne change PAS la dimension
âœ… change seulement la longueur (norme) du vecteur

ğŸ‘‰ 384 paramÃ¨tres restent 384 paramÃ¨tres

ğŸ”¹ Avec normalize_embeddings=True

Le modÃ¨le fait :
v_normalized = v / ||v||

RÃ©sultat :
||v_normalized|| = 1

â“ Pourquoi on fait Ã§a ?
Cosine similarity (dÃ©finition)
cos(u, v) = (u Â· v) / (||u|| * ||v||)

MAIS si ||u|| = 1 et ||v|| = 1 :
cos(u, v) = u Â· v


ğŸ’¥ Le dot product devient EXACTEMENT la cosine similarity 
Pourquoi	vitesse + stabilitÃ©
        """